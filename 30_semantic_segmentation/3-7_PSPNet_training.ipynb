{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 学習と検証の実施\n",
    "\n",
    "- 本ファイルでは、PSPNetの学習と検証の実施を行います。AWSのGPUマシンで計算します。\n",
    "- p2.xlargeで約12時間かかります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習目標\n",
    "\n",
    "1.\tPSPNetの学習と検証を実装できるようになる\n",
    "2.\tセマンティック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "\n",
    "- 本書に従い学習済みモデルのファイル「pspnet50_ADE20K.pth」をダウンロードし、フォルダ「weights」に用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# ファイルパスリスト作成\n",
    "rootpath = \"./data/VOCdevkit/VOC2007/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "# (RGB)の色の平均値と標準偏差\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークモデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了：学習済みの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ファインチューニングでPSPNetを作成\n",
    "# ADE20Kデータセットの学習済みモデルを使用、ADE20Kはクラス数が150です\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "# ADE20K学習済みパラメータをロード\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 分類用の畳み込み層を、出力数21のものにつけかえる\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(\n",
    "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "net.aux.classification = nn.Conv2d(\n",
    "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する。\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)\n",
    "\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "class PSPLoss(nn.Module):\n",
    "    \"\"\"PSPNetの損失関数のクラスです。\"\"\"\n",
    "\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight  # aux_lossの重み\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : PSPNetの出力(tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
    "\n",
    "        targets : [num_batch, 475, 475]\n",
    "            正解のアノテーション情報\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : テンソル\n",
    "            損失の値\n",
    "        \"\"\"\n",
    "\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化手法を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングなので、学習率は小さく\n",
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "# スケジューラーの設定\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 60\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・検証を実施する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                scheduler.step()  # 最適化schedulerの更新\n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "\n",
    "                \n",
    "                # multiple minibatchでのパラメータの更新\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    # 最後のネットワークを保存する\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------\n",
      "Epoch 1/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 10 || Loss: 0.1134 || 10iter: 14.2962 sec.\n",
      "イテレーション 20 || Loss: 0.1175 || 10iter: 11.1692 sec.\n",
      "イテレーション 30 || Loss: 0.1025 || 10iter: 11.1947 sec.\n",
      "イテレーション 40 || Loss: 0.1921 || 10iter: 11.3262 sec.\n",
      "イテレーション 50 || Loss: 0.1093 || 10iter: 11.2374 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.0957 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  67.1068 sec.\n",
      "-------------\n",
      "Epoch 2/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 60 || Loss: 0.0327 || 10iter: 7.7880 sec.\n",
      "イテレーション 70 || Loss: 0.0496 || 10iter: 9.7709 sec.\n",
      "イテレーション 80 || Loss: 0.1152 || 10iter: 9.4488 sec.\n",
      "イテレーション 90 || Loss: 0.0690 || 10iter: 9.1820 sec.\n",
      "イテレーション 100 || Loss: 0.2316 || 10iter: 9.0819 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.0918 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  53.8333 sec.\n",
      "-------------\n",
      "Epoch 3/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 110 || Loss: 0.0978 || 10iter: 5.0879 sec.\n",
      "イテレーション 120 || Loss: 0.0883 || 10iter: 9.0517 sec.\n",
      "イテレーション 130 || Loss: 0.1030 || 10iter: 9.0830 sec.\n",
      "イテレーション 140 || Loss: 0.0742 || 10iter: 9.1139 sec.\n",
      "イテレーション 150 || Loss: 0.0766 || 10iter: 9.0767 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:0.0842 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  51.7933 sec.\n",
      "-------------\n",
      "Epoch 4/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 160 || Loss: 0.0526 || 10iter: 3.1151 sec.\n",
      "イテレーション 170 || Loss: 0.0609 || 10iter: 9.1082 sec.\n",
      "イテレーション 180 || Loss: 0.0748 || 10iter: 9.1157 sec.\n",
      "イテレーション 190 || Loss: 0.0380 || 10iter: 9.1034 sec.\n",
      "イテレーション 200 || Loss: 0.0692 || 10iter: 9.1290 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:0.0927 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  51.9929 sec.\n",
      "-------------\n",
      "Epoch 5/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 210 || Loss: 0.0693 || 10iter: 1.1165 sec.\n",
      "イテレーション 220 || Loss: 0.0562 || 10iter: 9.1247 sec.\n",
      "イテレーション 230 || Loss: 0.0924 || 10iter: 9.1638 sec.\n",
      "イテレーション 240 || Loss: 0.0840 || 10iter: 9.1383 sec.\n",
      "イテレーション 250 || Loss: 0.0609 || 10iter: 9.1692 sec.\n",
      "イテレーション 260 || Loss: 0.2127 || 10iter: 9.1863 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:0.0896 ||Epoch_VAL_Loss:0.2063\n",
      "timer:  67.1373 sec.\n",
      "-------------\n",
      "Epoch 6/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 270 || Loss: 0.1097 || 10iter: 8.7650 sec.\n",
      "イテレーション 280 || Loss: 0.0733 || 10iter: 8.7015 sec.\n",
      "イテレーション 290 || Loss: 0.0554 || 10iter: 8.7352 sec.\n",
      "イテレーション 300 || Loss: 0.0709 || 10iter: 8.8587 sec.\n",
      "イテレーション 310 || Loss: 0.0688 || 10iter: 8.7860 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:0.0890 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.0101 sec.\n",
      "-------------\n",
      "Epoch 7/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 320 || Loss: 0.0494 || 10iter: 6.7810 sec.\n",
      "イテレーション 330 || Loss: 0.0984 || 10iter: 8.4931 sec.\n",
      "イテレーション 340 || Loss: 0.0871 || 10iter: 8.5552 sec.\n",
      "イテレーション 350 || Loss: 0.1258 || 10iter: 8.5173 sec.\n",
      "イテレーション 360 || Loss: 0.0958 || 10iter: 8.5142 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:0.0835 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.7390 sec.\n",
      "-------------\n",
      "Epoch 8/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 370 || Loss: 0.0857 || 10iter: 4.8002 sec.\n",
      "イテレーション 380 || Loss: 0.0669 || 10iter: 8.5252 sec.\n",
      "イテレーション 390 || Loss: 0.0849 || 10iter: 8.5113 sec.\n",
      "イテレーション 400 || Loss: 0.0981 || 10iter: 8.5462 sec.\n",
      "イテレーション 410 || Loss: 0.0657 || 10iter: 8.8285 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:0.0807 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  49.1430 sec.\n",
      "-------------\n",
      "Epoch 9/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 420 || Loss: 0.0398 || 10iter: 2.9979 sec.\n",
      "イテレーション 430 || Loss: 0.0600 || 10iter: 8.7773 sec.\n",
      "イテレーション 440 || Loss: 0.0330 || 10iter: 8.8297 sec.\n",
      "イテレーション 450 || Loss: 0.0350 || 10iter: 8.8043 sec.\n",
      "イテレーション 460 || Loss: 0.0591 || 10iter: 8.7736 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:0.0793 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.1620 sec.\n",
      "-------------\n",
      "Epoch 10/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 470 || Loss: 0.1076 || 10iter: 1.1163 sec.\n",
      "イテレーション 480 || Loss: 0.0498 || 10iter: 8.7897 sec.\n",
      "イテレーション 490 || Loss: 0.0577 || 10iter: 8.8024 sec.\n",
      "イテレーション 500 || Loss: 0.0686 || 10iter: 8.7903 sec.\n",
      "イテレーション 510 || Loss: 0.0283 || 10iter: 8.7951 sec.\n",
      "イテレーション 520 || Loss: 0.0407 || 10iter: 8.8208 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:0.0788 ||Epoch_VAL_Loss:0.2069\n",
      "timer:  64.6416 sec.\n",
      "-------------\n",
      "Epoch 11/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 530 || Loss: 0.0450 || 10iter: 8.8644 sec.\n",
      "イテレーション 540 || Loss: 0.0378 || 10iter: 8.8143 sec.\n",
      "イテレーション 550 || Loss: 0.0413 || 10iter: 8.8263 sec.\n",
      "イテレーション 560 || Loss: 0.0576 || 10iter: 8.7544 sec.\n",
      "イテレーション 570 || Loss: 0.0841 || 10iter: 8.7806 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:0.0928 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.1962 sec.\n",
      "-------------\n",
      "Epoch 12/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 580 || Loss: 0.0664 || 10iter: 6.8872 sec.\n",
      "イテレーション 590 || Loss: 0.0781 || 10iter: 8.7828 sec.\n",
      "イテレーション 600 || Loss: 0.0639 || 10iter: 8.7886 sec.\n",
      "イテレーション 610 || Loss: 0.1348 || 10iter: 8.7450 sec.\n",
      "イテレーション 620 || Loss: 0.0450 || 10iter: 8.8038 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:0.0724 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.1591 sec.\n",
      "-------------\n",
      "Epoch 13/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 630 || Loss: 0.0443 || 10iter: 5.4254 sec.\n",
      "イテレーション 640 || Loss: 0.0466 || 10iter: 10.8347 sec.\n",
      "イテレーション 650 || Loss: 0.0431 || 10iter: 11.1380 sec.\n",
      "イテレーション 660 || Loss: 0.0838 || 10iter: 10.0121 sec.\n",
      "イテレーション 670 || Loss: 0.0917 || 10iter: 12.5371 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:0.0781 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  63.9621 sec.\n",
      "-------------\n",
      "Epoch 14/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 680 || Loss: 0.0922 || 10iter: 3.8726 sec.\n",
      "イテレーション 690 || Loss: 0.0567 || 10iter: 12.6552 sec.\n",
      "イテレーション 700 || Loss: 0.1090 || 10iter: 11.1308 sec.\n",
      "イテレーション 710 || Loss: 0.0589 || 10iter: 9.3200 sec.\n",
      "イテレーション 720 || Loss: 0.0516 || 10iter: 11.8562 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:0.0784 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  62.2840 sec.\n",
      "-------------\n",
      "Epoch 15/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 730 || Loss: 0.1986 || 10iter: 1.1367 sec.\n",
      "イテレーション 740 || Loss: 0.0455 || 10iter: 10.4981 sec.\n",
      "イテレーション 750 || Loss: 0.1021 || 10iter: 16.3556 sec.\n",
      "イテレーション 760 || Loss: 0.1085 || 10iter: 14.4922 sec.\n",
      "イテレーション 770 || Loss: 0.0672 || 10iter: 12.6967 sec.\n",
      "イテレーション 780 || Loss: 0.1686 || 10iter: 12.6769 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:0.0747 ||Epoch_VAL_Loss:0.2029\n",
      "timer:  97.6301 sec.\n",
      "-------------\n",
      "Epoch 16/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 790 || Loss: 0.0421 || 10iter: 13.3388 sec.\n",
      "イテレーション 800 || Loss: 0.0336 || 10iter: 12.6362 sec.\n",
      "イテレーション 810 || Loss: 0.1427 || 10iter: 10.6012 sec.\n",
      "イテレーション 820 || Loss: 0.1356 || 10iter: 9.1305 sec.\n",
      "イテレーション 830 || Loss: 0.0399 || 10iter: 9.1024 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:0.0743 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  62.2782 sec.\n",
      "-------------\n",
      "Epoch 17/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 840 || Loss: 0.0776 || 10iter: 7.1999 sec.\n",
      "イテレーション 850 || Loss: 0.0627 || 10iter: 9.2003 sec.\n",
      "イテレーション 860 || Loss: 0.0462 || 10iter: 9.2415 sec.\n",
      "イテレーション 870 || Loss: 0.1841 || 10iter: 9.3622 sec.\n",
      "イテレーション 880 || Loss: 0.0647 || 10iter: 9.4711 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:0.0755 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  53.1669 sec.\n",
      "-------------\n",
      "Epoch 18/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 890 || Loss: 0.0745 || 10iter: 5.3291 sec.\n",
      "イテレーション 900 || Loss: 0.0377 || 10iter: 10.6523 sec.\n",
      "イテレーション 910 || Loss: 0.0509 || 10iter: 13.4161 sec.\n",
      "イテレーション 920 || Loss: 0.0507 || 10iter: 12.0485 sec.\n",
      "イテレーション 930 || Loss: 0.2073 || 10iter: 11.8338 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:0.0660 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  67.9374 sec.\n",
      "-------------\n",
      "Epoch 19/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 940 || Loss: 0.0698 || 10iter: 3.7885 sec.\n",
      "イテレーション 950 || Loss: 0.1084 || 10iter: 13.2916 sec.\n",
      "イテレーション 960 || Loss: 0.0786 || 10iter: 11.7403 sec.\n",
      "イテレーション 970 || Loss: 0.0389 || 10iter: 11.7339 sec.\n",
      "イテレーション 980 || Loss: 0.0504 || 10iter: 10.9359 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:0.0718 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  65.0171 sec.\n",
      "-------------\n",
      "Epoch 20/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 990 || Loss: 0.0562 || 10iter: 2.3661 sec.\n",
      "イテレーション 1000 || Loss: 0.1594 || 10iter: 13.7982 sec.\n",
      "イテレーション 1010 || Loss: 0.0620 || 10iter: 13.8990 sec.\n",
      "イテレーション 1020 || Loss: 0.0747 || 10iter: 13.9065 sec.\n",
      "イテレーション 1030 || Loss: 0.0662 || 10iter: 13.9904 sec.\n",
      "イテレーション 1040 || Loss: 0.0607 || 10iter: 14.0055 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:0.0641 ||Epoch_VAL_Loss:0.2036\n",
      "timer:  94.6089 sec.\n",
      "-------------\n",
      "Epoch 21/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1050 || Loss: 0.0649 || 10iter: 13.6131 sec.\n",
      "イテレーション 1060 || Loss: 0.0502 || 10iter: 14.0550 sec.\n",
      "イテレーション 1070 || Loss: 0.0876 || 10iter: 13.0369 sec.\n",
      "イテレーション 1080 || Loss: 0.0592 || 10iter: 9.7447 sec.\n",
      "イテレーション 1090 || Loss: 0.0774 || 10iter: 9.7445 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:0.0702 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  67.4067 sec.\n",
      "-------------\n",
      "Epoch 22/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1100 || Loss: 0.1005 || 10iter: 7.7712 sec.\n",
      "イテレーション 1110 || Loss: 0.0544 || 10iter: 9.8447 sec.\n",
      "イテレーション 1120 || Loss: 0.0617 || 10iter: 9.9255 sec.\n",
      "イテレーション 1130 || Loss: 0.0752 || 10iter: 10.0126 sec.\n",
      "イテレーション 1140 || Loss: 0.0631 || 10iter: 9.8369 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:0.0676 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  56.5594 sec.\n",
      "-------------\n",
      "Epoch 23/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1150 || Loss: 0.0588 || 10iter: 5.5743 sec.\n",
      "イテレーション 1160 || Loss: 0.0534 || 10iter: 12.6875 sec.\n",
      "イテレーション 1170 || Loss: 0.0839 || 10iter: 18.6184 sec.\n",
      "イテレーション 1180 || Loss: 0.0658 || 10iter: 36.5760 sec.\n",
      "イテレーション 1190 || Loss: 0.1092 || 10iter: 12.6021 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:0.0638 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  100.4698 sec.\n",
      "-------------\n",
      "Epoch 24/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1200 || Loss: 0.0321 || 10iter: 4.1997 sec.\n",
      "イテレーション 1210 || Loss: 0.0393 || 10iter: 12.6772 sec.\n",
      "イテレーション 1220 || Loss: 0.0594 || 10iter: 12.5771 sec.\n",
      "イテレーション 1230 || Loss: 0.0459 || 10iter: 12.6286 sec.\n",
      "イテレーション 1240 || Loss: 0.0629 || 10iter: 12.5964 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:0.0721 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  72.0574 sec.\n",
      "-------------\n",
      "Epoch 25/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1250 || Loss: 0.0429 || 10iter: 1.2092 sec.\n",
      "イテレーション 1260 || Loss: 0.0770 || 10iter: 11.0486 sec.\n",
      "イテレーション 1270 || Loss: 0.0479 || 10iter: 9.5246 sec.\n",
      "イテレーション 1280 || Loss: 0.1505 || 10iter: 9.7446 sec.\n",
      "イテレーション 1290 || Loss: 0.0632 || 10iter: 10.0900 sec.\n",
      "イテレーション 1300 || Loss: 0.0589 || 10iter: 9.7016 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:0.0709 ||Epoch_VAL_Loss:0.2085\n",
      "timer:  94.0956 sec.\n",
      "-------------\n",
      "Epoch 26/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1310 || Loss: 0.0485 || 10iter: 9.3246 sec.\n",
      "イテレーション 1320 || Loss: 0.0435 || 10iter: 9.1919 sec.\n",
      "イテレーション 1330 || Loss: 0.0564 || 10iter: 9.1727 sec.\n",
      "イテレーション 1340 || Loss: 0.0502 || 10iter: 9.6835 sec.\n",
      "イテレーション 1350 || Loss: 0.1293 || 10iter: 11.8819 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:0.0696 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  57.0096 sec.\n",
      "-------------\n",
      "Epoch 27/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1360 || Loss: 0.0311 || 10iter: 9.3641 sec.\n",
      "イテレーション 1370 || Loss: 0.0364 || 10iter: 9.7596 sec.\n",
      "イテレーション 1380 || Loss: 0.0567 || 10iter: 10.6895 sec.\n",
      "イテレーション 1390 || Loss: 0.0396 || 10iter: 12.4586 sec.\n",
      "イテレーション 1400 || Loss: 0.0419 || 10iter: 12.2641 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:0.0647 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  65.1958 sec.\n",
      "-------------\n",
      "Epoch 28/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1410 || Loss: 0.0539 || 10iter: 5.7099 sec.\n",
      "イテレーション 1420 || Loss: 0.0924 || 10iter: 10.0608 sec.\n",
      "イテレーション 1430 || Loss: 0.0907 || 10iter: 9.5540 sec.\n",
      "イテレーション 1440 || Loss: 0.1023 || 10iter: 9.2556 sec.\n",
      "イテレーション 1450 || Loss: 0.0467 || 10iter: 9.5907 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:0.0663 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  55.1641 sec.\n",
      "-------------\n",
      "Epoch 29/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1460 || Loss: 0.0275 || 10iter: 3.2058 sec.\n",
      "イテレーション 1470 || Loss: 0.0530 || 10iter: 9.7580 sec.\n",
      "イテレーション 1480 || Loss: 0.1030 || 10iter: 9.4827 sec.\n",
      "イテレーション 1490 || Loss: 0.0420 || 10iter: 9.4176 sec.\n",
      "イテレーション 1500 || Loss: 0.0573 || 10iter: 9.4716 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:0.0645 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  54.2245 sec.\n",
      "-------------\n",
      "Epoch 30/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1510 || Loss: 0.0683 || 10iter: 1.1745 sec.\n",
      "イテレーション 1520 || Loss: 0.0732 || 10iter: 9.4198 sec.\n",
      "イテレーション 1530 || Loss: 0.0580 || 10iter: 11.5690 sec.\n",
      "イテレーション 1540 || Loss: 0.0730 || 10iter: 11.2868 sec.\n",
      "イテレーション 1550 || Loss: 0.1012 || 10iter: 11.3667 sec.\n",
      "イテレーション 1560 || Loss: 0.0500 || 10iter: 11.2989 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:0.0750 ||Epoch_VAL_Loss:0.2090\n",
      "timer:  86.3203 sec.\n",
      "-------------\n",
      "Epoch 31/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1570 || Loss: 0.0639 || 10iter: 10.7545 sec.\n",
      "イテレーション 1580 || Loss: 0.0678 || 10iter: 11.5594 sec.\n",
      "イテレーション 1590 || Loss: 0.0556 || 10iter: 11.2884 sec.\n",
      "イテレーション 1600 || Loss: 0.0624 || 10iter: 11.3350 sec.\n",
      "イテレーション 1610 || Loss: 0.0485 || 10iter: 11.5435 sec.\n",
      "-------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:0.0599 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  64.5188 sec.\n",
      "-------------\n",
      "Epoch 32/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1620 || Loss: 0.0422 || 10iter: 8.7755 sec.\n",
      "イテレーション 1630 || Loss: 0.0606 || 10iter: 11.1500 sec.\n",
      "イテレーション 1640 || Loss: 0.0373 || 10iter: 11.2906 sec.\n",
      "イテレーション 1650 || Loss: 0.0315 || 10iter: 11.2976 sec.\n",
      "イテレーション 1660 || Loss: 0.0610 || 10iter: 11.2933 sec.\n",
      "-------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:0.0611 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  64.3422 sec.\n",
      "-------------\n",
      "Epoch 33/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1670 || Loss: 0.0575 || 10iter: 6.2691 sec.\n",
      "イテレーション 1680 || Loss: 0.0242 || 10iter: 11.3052 sec.\n",
      "イテレーション 1690 || Loss: 0.0757 || 10iter: 11.2589 sec.\n",
      "イテレーション 1700 || Loss: 0.0473 || 10iter: 11.0398 sec.\n",
      "イテレーション 1710 || Loss: 0.1090 || 10iter: 10.4214 sec.\n",
      "-------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:0.0624 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  62.0943 sec.\n",
      "-------------\n",
      "Epoch 34/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1720 || Loss: 0.0670 || 10iter: 3.2870 sec.\n",
      "イテレーション 1730 || Loss: 0.0579 || 10iter: 10.0016 sec.\n",
      "イテレーション 1740 || Loss: 0.0956 || 10iter: 10.5334 sec.\n",
      "イテレーション 1750 || Loss: 0.0527 || 10iter: 9.4575 sec.\n",
      "イテレーション 1760 || Loss: 0.0335 || 10iter: 8.5607 sec.\n",
      "-------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:0.0583 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  53.9154 sec.\n",
      "-------------\n",
      "Epoch 35/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1770 || Loss: 0.0494 || 10iter: 1.0841 sec.\n",
      "イテレーション 1780 || Loss: 0.0571 || 10iter: 8.5023 sec.\n",
      "イテレーション 1790 || Loss: 0.0343 || 10iter: 8.4903 sec.\n",
      "イテレーション 1800 || Loss: 0.0362 || 10iter: 8.4843 sec.\n",
      "イテレーション 1810 || Loss: 0.0803 || 10iter: 8.4953 sec.\n",
      "イテレーション 1820 || Loss: 0.0564 || 10iter: 8.5132 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:0.0643 ||Epoch_VAL_Loss:0.2068\n",
      "timer:  62.3712 sec.\n",
      "-------------\n",
      "Epoch 36/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1830 || Loss: 0.0848 || 10iter: 8.4933 sec.\n",
      "イテレーション 1840 || Loss: 0.0617 || 10iter: 8.5372 sec.\n",
      "イテレーション 1850 || Loss: 0.0306 || 10iter: 8.4983 sec.\n",
      "イテレーション 1860 || Loss: 0.0465 || 10iter: 8.5092 sec.\n",
      "イテレーション 1870 || Loss: 0.0767 || 10iter: 8.4693 sec.\n",
      "-------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:0.0593 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.5023 sec.\n",
      "-------------\n",
      "Epoch 37/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1880 || Loss: 0.0641 || 10iter: 6.6203 sec.\n",
      "イテレーション 1890 || Loss: 0.0651 || 10iter: 8.4893 sec.\n",
      "イテレーション 1900 || Loss: 0.0278 || 10iter: 8.4763 sec.\n",
      "イテレーション 1910 || Loss: 0.0616 || 10iter: 8.6643 sec.\n",
      "イテレーション 1920 || Loss: 0.0250 || 10iter: 9.1898 sec.\n",
      "-------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:0.0603 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  49.7122 sec.\n",
      "-------------\n",
      "Epoch 38/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1930 || Loss: 0.0441 || 10iter: 4.8720 sec.\n",
      "イテレーション 1940 || Loss: 0.0573 || 10iter: 8.7687 sec.\n",
      "イテレーション 1950 || Loss: 0.0460 || 10iter: 8.7150 sec.\n",
      "イテレーション 1960 || Loss: 0.0643 || 10iter: 8.6680 sec.\n",
      "イテレーション 1970 || Loss: 0.0508 || 10iter: 8.7477 sec.\n",
      "-------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:0.0565 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  49.4944 sec.\n",
      "-------------\n",
      "Epoch 39/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1980 || Loss: 0.0277 || 10iter: 2.9361 sec.\n",
      "イテレーション 1990 || Loss: 0.1323 || 10iter: 8.5283 sec.\n",
      "イテレーション 2000 || Loss: 0.0515 || 10iter: 8.4783 sec.\n",
      "イテレーション 2010 || Loss: 0.0332 || 10iter: 8.6232 sec.\n",
      "イテレーション 2020 || Loss: 0.0546 || 10iter: 8.9800 sec.\n",
      "-------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:0.0624 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  49.5202 sec.\n",
      "-------------\n",
      "Epoch 40/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2030 || Loss: 0.0799 || 10iter: 1.0921 sec.\n",
      "イテレーション 2040 || Loss: 0.0534 || 10iter: 8.6030 sec.\n",
      "イテレーション 2050 || Loss: 0.0614 || 10iter: 8.6828 sec.\n",
      "イテレーション 2060 || Loss: 0.0513 || 10iter: 8.7860 sec.\n",
      "イテレーション 2070 || Loss: 0.0507 || 10iter: 8.5830 sec.\n",
      "イテレーション 2080 || Loss: 0.0572 || 10iter: 8.7440 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:0.0587 ||Epoch_VAL_Loss:0.2045\n",
      "timer:  63.7157 sec.\n",
      "-------------\n",
      "Epoch 41/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2090 || Loss: 0.0712 || 10iter: 8.5412 sec.\n",
      "イテレーション 2100 || Loss: 0.0457 || 10iter: 8.6140 sec.\n",
      "イテレーション 2110 || Loss: 0.0919 || 10iter: 8.5731 sec.\n",
      "イテレーション 2120 || Loss: 0.0392 || 10iter: 8.5252 sec.\n",
      "イテレーション 2130 || Loss: 0.0714 || 10iter: 8.4793 sec.\n",
      "-------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:0.0622 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  48.7127 sec.\n",
      "-------------\n",
      "Epoch 42/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2140 || Loss: 0.0873 || 10iter: 6.6522 sec.\n",
      "イテレーション 2150 || Loss: 0.0584 || 10iter: 8.4973 sec.\n",
      "イテレーション 2160 || Loss: 0.0685 || 10iter: 8.5122 sec.\n",
      "イテレーション 2170 || Loss: 0.0212 || 10iter: 8.9514 sec.\n",
      "イテレーション 2180 || Loss: 0.0632 || 10iter: 8.5850 sec.\n",
      "-------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:0.0559 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  49.2487 sec.\n",
      "-------------\n",
      "Epoch 43/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2190 || Loss: 0.0818 || 10iter: 4.7893 sec.\n",
      "イテレーション 2200 || Loss: 0.0593 || 10iter: 8.5676 sec.\n",
      "イテレーション 2210 || Loss: 0.0764 || 10iter: 8.7257 sec.\n",
      "イテレーション 2220 || Loss: 0.0838 || 10iter: 8.8225 sec.\n",
      "イテレーション 2230 || Loss: 0.0329 || 10iter: 9.0449 sec.\n",
      "-------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:0.0558 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.4136 sec.\n",
      "-------------\n",
      "Epoch 44/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2240 || Loss: 0.0570 || 10iter: 3.2139 sec.\n",
      "イテレーション 2250 || Loss: 0.0550 || 10iter: 9.0869 sec.\n",
      "イテレーション 2260 || Loss: 0.0698 || 10iter: 9.2719 sec.\n",
      "イテレーション 2270 || Loss: 0.0268 || 10iter: 9.2827 sec.\n",
      "イテレーション 2280 || Loss: 0.1184 || 10iter: 9.2863 sec.\n",
      "-------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:0.0564 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  52.7928 sec.\n",
      "-------------\n",
      "Epoch 45/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2290 || Loss: 0.0790 || 10iter: 1.1538 sec.\n",
      "イテレーション 2300 || Loss: 0.0443 || 10iter: 9.3138 sec.\n",
      "イテレーション 2310 || Loss: 0.0314 || 10iter: 9.6087 sec.\n",
      "イテレーション 2320 || Loss: 0.0629 || 10iter: 8.8188 sec.\n",
      "イテレーション 2330 || Loss: 0.0448 || 10iter: 8.6718 sec.\n",
      "イテレーション 2340 || Loss: 0.0456 || 10iter: 8.8826 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:0.0596 ||Epoch_VAL_Loss:0.2037\n",
      "timer:  66.5603 sec.\n",
      "-------------\n",
      "Epoch 46/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2350 || Loss: 0.0694 || 10iter: 9.0678 sec.\n",
      "イテレーション 2360 || Loss: 0.0369 || 10iter: 8.8754 sec.\n",
      "イテレーション 2370 || Loss: 0.1007 || 10iter: 8.7503 sec.\n",
      "イテレーション 2380 || Loss: 0.1288 || 10iter: 8.8384 sec.\n",
      "イテレーション 2390 || Loss: 0.0218 || 10iter: 8.7756 sec.\n",
      "-------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:0.0592 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  50.4611 sec.\n",
      "-------------\n",
      "Epoch 47/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2400 || Loss: 0.0437 || 10iter: 6.8380 sec.\n",
      "イテレーション 2410 || Loss: 0.0599 || 10iter: 10.4362 sec.\n",
      "イテレーション 2420 || Loss: 0.0550 || 10iter: 11.9312 sec.\n",
      "イテレーション 2430 || Loss: 0.0663 || 10iter: 10.2629 sec.\n",
      "イテレーション 2440 || Loss: 0.0416 || 10iter: 12.0226 sec.\n",
      "-------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:0.0569 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  60.2439 sec.\n",
      "-------------\n",
      "Epoch 48/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2450 || Loss: 0.0614 || 10iter: 4.9599 sec.\n",
      "イテレーション 2460 || Loss: 0.0754 || 10iter: 8.7903 sec.\n",
      "イテレーション 2470 || Loss: 0.0588 || 10iter: 8.9639 sec.\n",
      "イテレーション 2480 || Loss: 0.0581 || 10iter: 12.0791 sec.\n",
      "イテレーション 2490 || Loss: 0.0495 || 10iter: 10.5470 sec.\n",
      "-------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:0.0597 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  56.9739 sec.\n",
      "-------------\n",
      "Epoch 49/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2500 || Loss: 0.0711 || 10iter: 3.6539 sec.\n",
      "イテレーション 2510 || Loss: 0.0605 || 10iter: 10.5930 sec.\n",
      "イテレーション 2520 || Loss: 0.0298 || 10iter: 10.4661 sec.\n",
      "イテレーション 2530 || Loss: 0.0390 || 10iter: 10.3356 sec.\n",
      "イテレーション 2540 || Loss: 0.0332 || 10iter: 10.7867 sec.\n",
      "-------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:0.0536 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  60.8238 sec.\n",
      "-------------\n",
      "Epoch 50/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2550 || Loss: 0.0340 || 10iter: 1.2274 sec.\n",
      "イテレーション 2560 || Loss: 0.0386 || 10iter: 10.3850 sec.\n",
      "イテレーション 2570 || Loss: 0.0469 || 10iter: 10.8242 sec.\n",
      "イテレーション 2580 || Loss: 0.0714 || 10iter: 10.1309 sec.\n",
      "イテレーション 2590 || Loss: 0.0427 || 10iter: 9.8935 sec.\n",
      "イテレーション 2600 || Loss: 0.0582 || 10iter: 9.9065 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:0.0567 ||Epoch_VAL_Loss:0.2088\n",
      "timer:  72.3069 sec.\n",
      "-------------\n",
      "Epoch 51/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2610 || Loss: 0.0928 || 10iter: 11.5152 sec.\n",
      "イテレーション 2620 || Loss: 0.0429 || 10iter: 10.2366 sec.\n",
      "イテレーション 2630 || Loss: 0.0769 || 10iter: 9.9633 sec.\n",
      "イテレーション 2640 || Loss: 0.0496 || 10iter: 9.9155 sec.\n",
      "イテレーション 2650 || Loss: 0.0413 || 10iter: 9.9264 sec.\n",
      "-------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:0.0554 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  58.6252 sec.\n",
      "-------------\n",
      "Epoch 52/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2660 || Loss: 0.0644 || 10iter: 8.6116 sec.\n",
      "イテレーション 2670 || Loss: 0.0508 || 10iter: 25974.0831 sec.\n",
      "イテレーション 2680 || Loss: 0.0374 || 10iter: 8.0375 sec.\n",
      "イテレーション 2690 || Loss: 0.0698 || 10iter: 8.0754 sec.\n",
      "イテレーション 2700 || Loss: 0.0321 || 10iter: 7.9398 sec.\n",
      "-------------\n",
      "epoch 52 || Epoch_TRAIN_Loss:0.0560 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  26014.1944 sec.\n",
      "-------------\n",
      "Epoch 53/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2710 || Loss: 0.0711 || 10iter: 4.4820 sec.\n",
      "イテレーション 2720 || Loss: 0.0569 || 10iter: 8.0814 sec.\n",
      "イテレーション 2730 || Loss: 0.0457 || 10iter: 8.0864 sec.\n",
      "イテレーション 2740 || Loss: 0.0535 || 10iter: 8.0694 sec.\n",
      "イテレーション 2750 || Loss: 0.0417 || 10iter: 8.1482 sec.\n",
      "-------------\n",
      "epoch 53 || Epoch_TRAIN_Loss:0.0616 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  46.0868 sec.\n",
      "-------------\n",
      "Epoch 54/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2760 || Loss: 0.0821 || 10iter: 2.8513 sec.\n",
      "イテレーション 2770 || Loss: 0.0675 || 10iter: 8.1811 sec.\n",
      "イテレーション 2780 || Loss: 0.0944 || 10iter: 8.2000 sec.\n",
      "イテレーション 2790 || Loss: 0.0241 || 10iter: 8.2759 sec.\n",
      "イテレーション 2800 || Loss: 0.0422 || 10iter: 8.2540 sec.\n",
      "-------------\n",
      "epoch 54 || Epoch_TRAIN_Loss:0.0561 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.0541 sec.\n",
      "-------------\n",
      "Epoch 55/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2810 || Loss: 0.0364 || 10iter: 1.0312 sec.\n",
      "イテレーション 2820 || Loss: 0.0476 || 10iter: 8.2938 sec.\n",
      "イテレーション 2830 || Loss: 0.1011 || 10iter: 8.3906 sec.\n",
      "イテレーション 2840 || Loss: 0.0478 || 10iter: 8.4115 sec.\n",
      "イテレーション 2850 || Loss: 0.0543 || 10iter: 8.3297 sec.\n",
      "イテレーション 2860 || Loss: 0.0413 || 10iter: 8.4005 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 55 || Epoch_TRAIN_Loss:0.0545 ||Epoch_VAL_Loss:0.2057\n",
      "timer:  61.4466 sec.\n",
      "-------------\n",
      "Epoch 56/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2870 || Loss: 0.0884 || 10iter: 8.4314 sec.\n",
      "イテレーション 2880 || Loss: 0.0717 || 10iter: 8.3736 sec.\n",
      "イテレーション 2890 || Loss: 0.0635 || 10iter: 8.4155 sec.\n",
      "イテレーション 2900 || Loss: 0.0578 || 10iter: 8.4234 sec.\n",
      "イテレーション 2910 || Loss: 0.0323 || 10iter: 8.4165 sec.\n",
      "-------------\n",
      "epoch 56 || Epoch_TRAIN_Loss:0.0640 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.9567 sec.\n",
      "-------------\n",
      "Epoch 57/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2920 || Loss: 0.0319 || 10iter: 6.6283 sec.\n",
      "イテレーション 2930 || Loss: 0.0416 || 10iter: 8.3925 sec.\n",
      "イテレーション 2940 || Loss: 0.0686 || 10iter: 8.3885 sec.\n",
      "イテレーション 2950 || Loss: 0.1068 || 10iter: 8.3926 sec.\n",
      "イテレーション 2960 || Loss: 0.1047 || 10iter: 8.4205 sec.\n",
      "-------------\n",
      "epoch 57 || Epoch_TRAIN_Loss:0.0543 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.9617 sec.\n",
      "-------------\n",
      "Epoch 58/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2970 || Loss: 0.0353 || 10iter: 4.7174 sec.\n",
      "イテレーション 2980 || Loss: 0.0420 || 10iter: 8.4045 sec.\n",
      "イテレーション 2990 || Loss: 0.0618 || 10iter: 8.3796 sec.\n",
      "イテレーション 3000 || Loss: 0.0498 || 10iter: 8.4045 sec.\n",
      "イテレーション 3010 || Loss: 0.0370 || 10iter: 8.4095 sec.\n",
      "-------------\n",
      "epoch 58 || Epoch_TRAIN_Loss:0.0544 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.9248 sec.\n",
      "-------------\n",
      "Epoch 59/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 3020 || Loss: 0.0329 || 10iter: 2.8763 sec.\n",
      "イテレーション 3030 || Loss: 0.0509 || 10iter: 8.4155 sec.\n",
      "イテレーション 3040 || Loss: 0.0618 || 10iter: 8.4005 sec.\n",
      "イテレーション 3050 || Loss: 0.0536 || 10iter: 8.3547 sec.\n",
      "イテレーション 3060 || Loss: 0.0805 || 10iter: 8.4155 sec.\n",
      "-------------\n",
      "epoch 59 || Epoch_TRAIN_Loss:0.0535 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  47.8809 sec.\n",
      "-------------\n",
      "Epoch 60/60\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 3070 || Loss: 0.0329 || 10iter: 1.0412 sec.\n",
      "イテレーション 3080 || Loss: 0.1011 || 10iter: 8.4145 sec.\n",
      "イテレーション 3090 || Loss: 0.2189 || 10iter: 8.4195 sec.\n",
      "イテレーション 3100 || Loss: 0.0476 || 10iter: 8.4204 sec.\n",
      "イテレーション 3110 || Loss: 0.0520 || 10iter: 8.3985 sec.\n",
      "イテレーション 3120 || Loss: 0.0839 || 10iter: 8.4234 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 60 || Epoch_TRAIN_Loss:0.0579 ||Epoch_VAL_Loss:0.2056\n",
      "timer:  61.7668 sec.\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する\n",
    "num_epochs = 60\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
